{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d12897d-0f6c-43c6-9698-794d1d7b4cde",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89c7677-a243-4fd8-92ea-a70e0837c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70555f-49f7-4e0e-bda5-26eac5d89d3f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416f975-9bd1-455a-9b3e-fdbb2398eb48",
   "metadata": {},
   "source": [
    "## Uni-directional LSTM (without attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419a52e-9c60-4384-8c4a-55281a560182",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19b7224-bb17-4be3-9c5b-cc1022edd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder1(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        return hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c18aa-ab0c-4eff-a4f5-2f9e5d2f00ac",
   "metadata": {},
   "source": [
    "### Dcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0080a-c4b4-40fa-b74b-638eec1e4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder1(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
    "        return prediction, hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93ab0f-d957-466c-9c26-b9883954b2bb",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e4148-185c-455c-8576-399fc0e7a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq1(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if use_teacher_force else top1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb897f-bef7-4a24-af6a-07b24d99e8c5",
   "metadata": {},
   "source": [
    "## Uni-directional LSTM (with attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdd0c5-5657-4522-97b0-8c4a836690cf",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4fe6ca-1561-47ef-9287-e100912db948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden.squeeze(0), cell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144eaa8c-85c0-493b-975c-25d8cc711841",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d834c9e-0c3c-4da9-8159-dda735634ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention2(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_a = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
    "        self.v_a = nn.Parameter(torch.rand(dec_hid_dim)) # same as doing nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        self.neg_inf = torch.tensor(-1e7, device=device)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, attention_mask):\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        #attention_mask = [batch_size, src_len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len-1 times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim]\n",
    "        \n",
    "        # attention scoring function - part 1 - tanh(W_a[s;h])\n",
    "        energy = torch.tanh(self.W_a(torch.cat((hidden, encoder_outputs), dim=2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v_a.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "        \n",
    "        # attention scoring function - part 2 - v_a(tanh(W_a[s;h]))\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "\n",
    "        # before computing the softmax, set attention to pad tokens to -infinity\n",
    "        attention[attention_mask] = self.neg_inf\n",
    "\n",
    "        # attention scoring function - part 2 - softmax(v_a(tanh(W_a[s;h])))\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb72155-aa86-4402-83e1-71f0e66887c5",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d2d0b-7ea3-406e-bb50-403d326a074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder2(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(enc_hid_dim + emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(enc_hid_dim + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs, attention_mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        #attention_mask = [batch_size, src_len]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        # get the attention probabilities\n",
    "        attention_weights = self.attention(hidden, encoder_outputs, attention_mask)\n",
    "                \n",
    "        #attention_weights = [batch size, src len]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim]\n",
    "        # perform weighted sum of encoder hidden states to get attention output\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim]\n",
    "        # concatenate the attention outputs (or context vectors) with the current decoder input\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim) + emb dim]\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        # classification over the entire word vocabulary\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), cell, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b57839-9c93-4599-81d1-3f1fdba37d80",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e1c88-cb07-4142-a07d-275605d050b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq2(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # create attention mask, set attention to pad tokens to -infinity \n",
    "        src_stoi = src_vocab.get_stoi()\n",
    "        attention_mask = (src == src_stoi[\"<pad>\"]).transpose(0, 1)\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0]-1, src.shape[0])\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs, attention_mask)\n",
    "            \n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:, t-1, :] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs, all_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d611a2-ce71-440b-97ac-040b629e49db",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM (with attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634cd29-8206-4440-b7c7-43a714f16417",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ba1f7a-8167-4635-9651-84405761415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout, bidirectional):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ### your code here ###\n",
    "        self.fc_hidden = nn.Linear(enc_hid_dim*2, enc_hid_dim)\n",
    "        self.fc_cell = nn.Linear(enc_hid_dim*2, enc_hid_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        encoder_outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # encoder_outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # encoder_outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "\n",
    "        ### your code here ###\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        \n",
    "        # hidden, cell = [num_layers, batch_size, enc_hid_dim]\n",
    "\n",
    "        return encoder_outputs, hidden.squeeze(0), cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfd9a0-e455-4656-8242-3edc61b9eefe",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661692a4-b3ff-4162-95fa-6885ae0ddeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim) ### your code here ###\n",
    "        self.v_a = nn.Parameter(torch.rand(dec_hid_dim)) # same as doing nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        self.neg_inf = torch.tensor(-1e7, device=device)\n",
    "        \n",
    "    def forward(self, encoder_outputs, hidden, cell, attention_mask):\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * num directions]\n",
    "        #attention_mask = [batch_size, src_len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len-1 times\n",
    "        repeat_dec_hidden = hidden.unsqueeze(0).repeat(src_len, 1, 1) ### your code here ###\n",
    "\n",
    "        #repeat_dec_hidden = [src len, batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * num directions]\n",
    "        \n",
    "        # attention scoring function - part 1 - tanh(W_a[s;h])\n",
    "        energy = torch.tanh(self.W_a(torch.cat((repeat_dec_hidden, encoder_outputs), dim=2)))\n",
    "        \n",
    "        #energy = [src len, batch size, dec hid dim]\n",
    "\n",
    "        # permute energy tensor to get right dim order before applying torch.bmm\n",
    "        energy = energy.permute(1,2,0) ### your code here ###\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        # repeat v \n",
    "        v = self.v_a.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "        \n",
    "        # attention scoring function - part 2 - v_a(tanh(W_a[s;h]))\n",
    "        # bmm docs: If mat1 is a (b×n×m) tensor, mat2 is a (b×m×p) tensor, out will be a (b×n×p) tensor.\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "\n",
    "        # before computing the softmax, set attention to pad tokens to -infinity\n",
    "        attention[attention_mask] = self.neg_inf\n",
    "\n",
    "        # attention scoring function - part 2 - softmax(v_a(tanh(W_a[s;h])))\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae7c17-ae2d-4b86-bc3a-347225e117aa",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d060771-2fd7-4150-ae54-a3d5a0ddfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim + enc_hid_dim*2, dec_hid_dim, n_layers, dropout=dropout)  ### your code here ###\n",
    "        \n",
    "        self.fc_out = nn.Linear(enc_hid_dim*2 + dec_hid_dim + emb_dim, output_dim)  ### your code here ###\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, encoder_outputs, hidden, cell, attention_mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #cell = [num layers, batch size, dec hid him]\n",
    "        #attention_mask = [batch_size, src_len]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        # get the attention probabilities\n",
    "        attention_weights = self.attention(encoder_outputs, hidden, cell, attention_mask)\n",
    "                \n",
    "        #attention_weights = [batch size, src len]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "\n",
    "        # perform weighted sum of encoder hidden states to get attention output\n",
    "        # this results in our context vectors\n",
    "        context_vectors = torch.bmm(attention_weights, encoder_outputs)\n",
    "        \n",
    "        #context_vectors = [batch size, 1, enc hid dim]\n",
    "        \n",
    "        context_vectors = context_vectors.permute(1, 0, 2)\n",
    "        \n",
    "        #context_vector = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        # concatenate the attention outputs (or context vectors) with the current decoder input\n",
    "        rnn_input = torch.cat((embedded, context_vectors), dim=2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "\n",
    "        output, (hidden, _) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and num directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        \n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        context_vectors = context_vectors.squeeze(0)\n",
    "        \n",
    "        # classification over the entire word vocabulary\n",
    "        prediction = self.fc_out(torch.cat((output, context_vectors, embedded), dim=1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cff0e-5929-4c99-93d6-b7ef0d385240",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5e166-299e-46a3-8e63-6a1de9ca3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # create attention mask, set attention to pad tokens to -infinity \n",
    "        src_stoi = src_vocab.get_stoi()\n",
    "        attention_mask = (src == src_stoi[\"<pad>\"]).transpose(0, 1)\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0] - 1, src.shape[0])\n",
    "\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden and cell contain relevant information from forward and backward passes\n",
    "        # obtained through applying nn.Linear() to each\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        # print(\"after encoder encoder_outputs, (hidden, cell):\", encoder_outputs.shape, hidden.shape, cell.shape)\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # print(f\"iter: {t}\")\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, attention_weights = self.decoder(input, encoder_outputs, hidden, cell, attention_mask) #<--------------\n",
    "\n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:, t - 1, :] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs, all_attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
